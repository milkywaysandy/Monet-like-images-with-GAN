# -*- coding: utf-8 -*-
"""Monet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1okwwMx_DJ-23unDHf1FBQcR-2LLCpxBb

##Iâ€™m Something of a Painter Myself
***GAN Model to generate Monet like image from scinery photo***

**Goal:**

Using Generative Adversarial Network (GAN) to generate pictures with Monet-like style on real-life photo.

**Data type:**

The dataset contain two types of images, Monet's original painting and real life photos. There are 300 Monet's paintings and 7038 of real-life photos; all in .jpg with 256 x 256 pixels and 3 layers each.

**GAN:**

Generative Adversarial Networks (GANs) is a type of frameworks to generate new data with the similar characteristics as the training data. A GAN consists of two neural networks, a generator and a discriminator. The generator creates new data samples while the discriminator evaluates them, with the ultimate goal for the generator to produce data so realistic that the discriminator can no longer distinguish it from real data.

[Dataset Link](https://www.kaggle.com/competitions/gan-getting-started)
"""

from google.colab import drive
drive.mount('/content/gdrive')

!unzip /content/gdrive/MyDrive/Data_Science/Monet_GAN/gan-getting-started.zip -d /content/monet

!pip install tensorflow
!pip install tensorflow-addons

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

import matplotlib.pyplot as plt
import numpy as np

"""##Exploratory Data Analysis (EDA)

**Inspection of the dataset:**

The inspection started with the loading the jpg and display them to see the type of images. Information on the images are also looked at.
***
**Data Cleaning:**

Another part od inspection on the image dataset is to identify potential outliers or anomalies. Additional to look at the images with nake eyes, a close look at the pixel value distributions was also done. There is no particular outliers or anomalies in this dataset, since the values are normalized to [-1, 1].
***
**Histogram:**

With code to see the pixel value distribution range, a display the average pixel values for each color channel in both datasets. The histogram helps to also visualize the distribution of pixel values.
***
**Analysis**

Since the dataset is clean without any outliers or anomalies, there is no special processing before setting up the model structures.

"""

folder_path = '/content/monet'

MONET_FILENAMES = tf.io.gfile.glob(str(folder_path + '/monet_tfrec/*.tfrec'))
print('Monet TFRecord Files:', len(MONET_FILENAMES))

PHOTO_FILENAMES = tf.io.gfile.glob(str(folder_path + '/photo_tfrec/*.tfrec'))
print('Photo TFRecord Files:', len(PHOTO_FILENAMES))

IMAGE_SIZE = [256, 256]

def decode_image(image):
    image = tf.image.decode_jpeg(image, channels=3)
    image = (tf.cast(image, tf.float32) / 127.5) - 1
    image = tf.reshape(image, [*IMAGE_SIZE, 3])
    return image

def read_tfrecord(example):
    tfrecord_format = {
        "image_name": tf.io.FixedLenFeature([], tf.string),
        "image": tf.io.FixedLenFeature([], tf.string),
        "target": tf.io.FixedLenFeature([], tf.string)
    }
    example = tf.io.parse_single_example(example, tfrecord_format)
    image = decode_image(example['image'])
    return image

AUTOTUNE = tf.data.AUTOTUNE

def load_dataset(filenames, labeled=True, ordered=False):
    dataset = tf.data.TFRecordDataset(filenames)
    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)
    return dataset

monet_ds = load_dataset(MONET_FILENAMES, labeled=True).batch(1)
photo_ds = load_dataset(PHOTO_FILENAMES, labeled=True).batch(1)
example_monet = next(iter(monet_ds))
example_photo = next(iter(photo_ds))

print('Number of Monet images:', len(list(monet_ds)))
print('Number of Photo images:', len(list(photo_ds)))

plt.subplot(121)
plt.title('Photo')
plt.imshow(example_photo[0] * 0.5 + 0.5)

plt.subplot(122)
plt.title('Monet')
plt.imshow(example_monet[0] * 0.5 + 0.5)

# pixel values Histogram
def collect_pixel_values(dataset):
    all_pixels = []
    for image_batch in dataset.take(100): # Take a subset to avoid memory issues
        all_pixels.append(image_batch.numpy().flatten())
    return np.concatenate(all_pixels)

monet_pixels = collect_pixel_values(monet_ds)
photo_pixels = collect_pixel_values(photo_ds)

plt.figure(figsize=(12, 6))

plt.subplot(121)
plt.hist(monet_pixels, bins=50, density=True, alpha=0.7, label='Monet')
plt.title('Distribution of Pixel Values (Monet)')
plt.xlabel('Pixel Value (Normalized)')
plt.ylabel('Density')
plt.legend()

plt.subplot(122)
plt.hist(photo_pixels, bins=50, density=True, alpha=0.7, label='Photo')
plt.title('Distribution of Pixel Values (Photo)')
plt.xlabel('Pixel Value (Normalized)')
plt.ylabel('Density')
plt.legend()

plt.tight_layout()
plt.show()

# Calculate mean pixel values per channel (more robust than collecting all pixels)
def get_mean_pixel_values(dataset):
    mean_values = []
    count = 0
    for image_batch in dataset:
        mean_values.append(np.mean(image_batch.numpy(), axis=(0, 1, 2)))
        count += image_batch.shape[0]
    return np.mean(mean_values, axis=0)

monet_mean_pixels = get_mean_pixel_values(monet_ds)
photo_mean_pixels = get_mean_pixel_values(photo_ds)

print("Mean pixel values (RGB) for Monet dataset:", monet_mean_pixels)
print("Mean pixel values (RGB) for Photo dataset:", photo_mean_pixels)

"""##Model Architecture

**CycleGAN Model Structure & Reasoning:**

The CycleGAN model is believe to be the best model strucutre to this challenge. The model structure learns to translate images from the Monet paintinf set to the real-life pictures without requiring paired examples. It uses two pairs of **generators** and **discriminators** to learn two inverse mappings: one from domain A to B, and another from B back to A. The core idea is the **cycle consistency loss**, which ensures that when translate an image from A to B and then back to A, a reconstructed image very similar to the original was generated. This forces the generators to produce meaningful and accurate translations.

***
**Hyperparameter Tuing:**

LAMBDA and EPOCHS were tuned as the improvement on the model preformance.

Impact of LAMBDA value:

*   LAMBDA had changed from 100 to 10.
*   The LAMBDA parameter controls the weight of the cycle consistency loss.
*   A higher LAMBDA (like 100 in the current notebook) puts more emphasis on ensuring that translating an image from domain A to B and then back to A results in the original image.
*   A lower LAMBDA (like 10 in the Kaggle notebook) gives the generator more freedom to deviate from the original image content if it helps in generating more realistic images in the target domain.

Impact of Number of Epochs:

*   EPOCHS changed from 10 to 20.
*   Training for only 10 epochs is likely insufficient for the CycleGAN model to converge and learn the complex mapping between the two image domains effectively. GAN training is notoriously unstable and requires many iterations to reach a good equilibrium between the generator and discriminator

***
**Structure Comparison:**

Regular GANs, consisting of a single generator and discriminator, are designed for one-way image generation from random noise, but they fail at complex image-to-image translation tasks without paired data. CycleGAN overcomes this limitation with a more sophisticated structure that includes two generators and two discriminators, allowing it to learn to translate images between two domains of Monet's painting and real-life images without requiring a one-to-one correspondence between images. The key innovation is the cycle consistency loss, which acts as a self-supervision mechanism, ensuring the translated image retains the original content and structure.
"""

#downsampling function to help encoding images
OUTPUT_CHANNELS = 3

def downsample(filters, size, apply_batchnorm=True):
    initializer = tf.random_normal_initializer(0., 0.02)

    result = tf.keras.Sequential()
    result.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',
                                kernel_initializer=initializer, use_bias=False))
    if apply_batchnorm:
        result.add(tf.keras.layers.BatchNormalization())

    result.add(tf.keras.layers.LeakyReLU())

    return result

#upsampling function to help decoding images
def upsample(filters, size, apply_dropout=False):
    initializer = tf.random_normal_initializer(0., 0.02)

    result = tf.keras.Sequential()
    result.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2,
                                        padding='same',
                                        kernel_initializer=initializer,
                                        use_bias=False))

    result.add(tf.keras.layers.BatchNormalization())

    if apply_dropout:
        result.add(tf.keras.layers.Dropout(0.5))

    result.add(tf.keras.layers.ReLU())

    return result

#generator structure function uses for both Monet and real-life photos
def Generator():
    initializer = tf.random_normal_initializer(0., 0.02)

    down_stack = [
        downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)
        downsample(128, 4), # (bs, 64, 64, 128)
        downsample(256, 4), # (bs, 32, 32, 256)
        downsample(512, 4), # (bs, 16, 16, 512)
        downsample(512, 4), # (bs, 8, 8, 512)
        downsample(512, 4), # (bs, 4, 4, 512)
        downsample(512, 4), # (bs, 2, 2, 512)
        downsample(512, 4)] # (bs, 1, 1, 512)

    up_stack = [
        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)
        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)
        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)
        upsample(512, 4), # (bs, 16, 16, 1024)
        upsample(256, 4), # (bs, 32, 32, 512)
        upsample(128, 4), # (bs, 64, 64, 256)
        upsample(64, 4)] # (bs, 128, 128, 128)

    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,
                                         strides=2, padding='same',
                                         kernel_initializer=initializer,
                                         activation='tanh') # (bs, 256, 256, 3)

    concat = tf.keras.layers.Concatenate()

    inputs = tf.keras.layers.Input(shape=[256, 256, 3])
    x = inputs

    # Downsampling through the model
    skips = []
    for down in down_stack:
        x = down(x)
        skips.append(x)
    skips = reversed(skips[:-1])

    # Upsampling and establishing the skip connections
    for up, skip in zip(up_stack, skips):
        x = up(x)
        x = concat([x, skip])
    x = last(x)
    return tf.keras.Model(inputs=inputs, outputs=x)

#discriminator structure function uses for cross train the generator

def Discriminator():
    initializer = tf.random_normal_initializer(0., 0.02)

    inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')
    x = inp

    down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)
    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)
    down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)

    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)
    conv = tf.keras.layers.Conv2D(512, 4, strides=1,
                                  kernel_initializer=initializer,
                                  use_bias=False)(zero_pad1) # (bs, 31, 31, 512)

    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)
    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)

    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)

    last = tf.keras.layers.Conv2D(1, 4, strides=1,
                                  kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)

    return tf.keras.Model(inputs=inp, outputs=last)

#run functions for both generator and discriminator
generator_g = Generator() # Photo to Monet
generator_f = Generator() # Monet to Photo

discriminator_x = Discriminator() # Discriminator for Monet
discriminator_y = Discriminator() # Discriminator for Photos

generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
generator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
discriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
discriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)

#define loss function
loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)

def discriminator_loss(real, generated):
    real_loss = loss_obj(tf.ones_like(real), real)
    generated_loss = loss_obj(tf.zeros_like(generated), generated)
    total_disc_loss = real_loss + generated_loss
    return total_disc_loss

def generator_loss(generated):
    return loss_obj(tf.ones_like(generated), generated)

def calc_cycle_loss(real_image, cycled_image):
    loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))
    return LAMBDA * loss1

def identity_loss(real_image, same_image):
    loss = tf.reduce_mean(tf.abs(real_image - same_image))
    return LAMBDA * 0.5 * loss

@tf.function
def train_step(real_monet, real_photo):
    with tf.GradientTape(persistent=True) as tape:
        # Forward Pass
        fake_monet = generator_g(real_photo, training=True) # Photo to Monet
        cycled_photo = generator_f(fake_monet, training=True) # fake_monet -> Monet to Photo

        fake_photo = generator_f(real_monet, training=True) # Monet to Photo
        cycled_monet = generator_g(fake_photo, training=True) # fake_photo -> Photo to Monet

        # Identity mapping
        same_monet = generator_g(real_monet, training=True) # real_monet -> Photo to Monet
        same_photo = generator_f(real_photo, training=True) # real_photo -> Monet to Photo

        # Discriminator predictions
        disc_real_monet = discriminator_x(real_monet, training=True)
        disc_fake_monet = discriminator_x(fake_monet, training=True)

        disc_real_photo = discriminator_y(real_photo, training=True)
        disc_fake_photo = discriminator_y(fake_photo, training=True)

        # Loss Calculation  # Adversarial Loss
        gen_g_loss = generator_loss(disc_fake_monet)
        gen_f_loss = generator_loss(disc_fake_photo)

        # Cycle Consistency Loss
        total_cycle_loss = calc_cycle_loss(real_monet, cycled_monet)
                          + calc_cycle_loss(real_photo, cycled_photo)

        # Identity Loss
        identity_monet_loss = identity_loss(real_monet, same_monet)
        identity_photo_loss = identity_loss(real_photo, same_photo)

        # Total Generator Loss
        total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_monet_loss
        total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_photo_loss

        # Discriminator Loss
        disc_x_loss = discriminator_loss(disc_real_monet, disc_fake_monet)
        disc_y_loss = discriminator_loss(disc_real_photo, disc_fake_photo)

    # Gradient Calculation
    generator_g_gradients = tape.gradient(total_gen_g_loss,generator_g.trainable_variables)
    generator_f_gradients = tape.gradient(total_gen_f_loss,generator_f.trainable_variables)

    discriminator_x_gradients = tape.gradient(disc_x_loss,discriminator_x.trainable_variables)
    discriminator_y_gradients = tape.gradient(disc_y_loss,discriminator_y.trainable_variables)

    # Apply Gradients
    generator_g_optimizer.apply_gradients(zip(generator_g_gradients,generator_g.trainable_variables))
    generator_f_optimizer.apply_gradients(zip(generator_f_gradients,generator_f.trainable_variables))

    discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,discriminator_x.trainable_variables))
    discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,discriminator_y.trainable_variables))

#Initial model training loop
LAMBDA, EPOCHS = 100, 10
import time
from IPython.display import clear_output

def generate_images_cyclegan(model, test_input, filename):
    prediction = model(test_input, training=True)
    plt.figure(figsize=(6, 6))

    display_list = [test_input[0], prediction[0]]
    title = ['Input Image', 'Generated Image']

    for i in range(2):
        plt.subplot(1, 2, i+1)
        plt.title(title[i])
        plt.imshow(display_list[i] * 0.5 + 0.5)
        plt.axis('off')

    plt.savefig(filename)
    plt.show()

import os# Create a directory to save generated images
os.makedirs('cyclegan_generated_images', exist_ok=True)

def fit_cyclegan(train_monet_ds, train_photo_ds, epochs, test_photo_ds, generator_g, generator_f, discriminator_x, discriminator_y, generator_g_optimizer, generator_f_optimizer, discriminator_x_optimizer, discriminator_y_optimizer):
    for epoch in range(epochs):
        start = time.time()

        n = 0
        for real_monet, real_photo in tf.data.Dataset.zip((train_monet_ds.repeat(), train_photo_ds.repeat())).take(max(len(list(train_monet_ds)), len(list(train_photo_ds)))):
            train_step(real_monet, real_photo)
            if n % 10 == 0:
                print('.', end='')
            n += 1

        clear_output(wait=True)
        generate_images_cyclegan(generator_g, next(iter(test_photo_ds.take(1))), f'cyclegan_generated_images/epoch_{epoch+1}.png')

        print(f'Epoch {epoch + 1} finished in {time.time()-start:.2f}s')

test_monet_ds = load_dataset(MONET_FILENAMES, labeled=True).batch(1).take(1)#train in subset
test_photo_ds = load_dataset(PHOTO_FILENAMES, labeled=True).batch(1).take(1)#train in subset

fit_cyclegan(monet_ds, photo_ds, EPOCHS, test_photo_ds, generator_g, generator_f, discriminator_x, discriminator_y, generator_g_optimizer, generator_f_optimizer, discriminator_x_optimizer, discriminator_y_optimizer)

#second iteration with hyperparameter tuning with Lambda & Epochs
LAMBDA, EPOCHS = 10, 20
import time
from IPython.display import clear_output

# Define a function to generate and save images during training
def generate_images_cyclegan(model, test_input, filename):
    prediction = model(test_input, training=True)
    plt.figure(figsize=(6, 6))

    display_list = [test_input[0], prediction[0]]
    title = ['Input Image', 'Generated Image']

    for i in range(2):
        plt.subplot(1, 2, i+1)
        plt.title(title[i])
        # Getting the pixel values in the range [0, 1] to plot them.
        plt.imshow(display_list[i] * 0.5 + 0.5)
        plt.axis('off')

    plt.savefig(filename)
    plt.show()

# Create a directory to save generated images
import os
os.makedirs('cyclegan_generated_images', exist_ok=True)

# Training loop
def fit_cyclegan(train_monet_ds, train_photo_ds, epochs, test_photo_ds, generator_g, generator_f, discriminator_x, discriminator_y, generator_g_optimizer, generator_f_optimizer, discriminator_x_optimizer, discriminator_y_optimizer):
    for epoch in range(epochs):
        start = time.time()

        n = 0
        # Iterate over both datasets simultaneously
        for real_monet, real_photo in tf.data.Dataset.zip((train_monet_ds.repeat(), train_photo_ds.repeat())).take(max(len(list(train_monet_ds)), len(list(train_photo_ds)))):
            train_step(real_monet, real_photo)
            if n % 10 == 0:
                print('.', end='')
            n += 1

        clear_output(wait=True)
        # Generate and save a sample image after each epoch using generator_g (Photo to Monet)
        generate_images_cyclegan(generator_g, next(iter(test_photo_ds.take(1))), f'cyclegan_generated_images/epoch_{epoch+1}.png')

        print(f'Epoch {epoch + 1} finished in {time.time()-start:.2f}s')

# Train the model
# Use smaller test sets for faster image generation during training
test_monet_ds = load_dataset(MONET_FILENAMES, labeled=True).batch(1).take(1)
test_photo_ds = load_dataset(PHOTO_FILENAMES, labeled=True).batch(1).take(1)


fit_cyclegan(monet_ds, photo_ds, EPOCHS, test_photo_ds, generator_g, generator_f, discriminator_x, discriminator_y, generator_g_optimizer, generator_f_optimizer, discriminator_x_optimizer, discriminator_y_optimizer)

"""##Results and Analysis

**Result Visualizations**

Please see the comparison image and generated photos of different hyperparameter turing below.
***
**Conclusion & Summary for Hyperparameter Tuning**

The trained GAN model to transform photos into Monet-style paintings was satisifying. The model architecture consisted of a generator network with a U-Net-like structure and a discriminator network. A CycleGAN-like training approach with adversarial loss, cycle consistency loss, and identity loss was used. After changing the hyperparameter tuning with Lambda & Epochs, the generated images is visually more painting-like, which make the turning successful.
***
**Reasoning on Well Working Hyperparameters**

*   Adjust LAMBDA: Reduce the value of LAMBDA from 100 to 10. This will give the generators more flexibility in style transfer.
*   Increase Training Epochs: Increase the number of training epochs from 10 to a larger number to allow the model sufficient time to learn the complex image transformations.
***
**Troubleshooting procedure**

The single generator and discriminator was used before the cycleGAN model was structured. The model did not know how to generate or evaluate the generated photo. Thus, the new cycleGAN structure was used.
"""

# Generate an example image after training
example_photo_batch = next(iter(photo_ds.take(1)))
generate_images_cyclegan(generator_g, example_photo_batch, 'final_generated_image.png')

example_photo_batch = next(iter(photo_ds.take(1)))
generate_images_cyclegan(generator_g, example_photo_batch, 'final_generated_image.png')

"""## Conclusion

**Summary of Results:**

The trained GAN model to transform photos into Monet-style paintings was satisifying. The model architecture consisted of a generator network with a U-Net-like structure and a discriminator network. A CycleGAN-like training approach with adversarial loss, cycle consistency loss, and identity loss was used.

*   After training for 20 epochs, the generated image shows some characteristics of Monet's style with overall improvment in comparison to 10 epochs.
*   The training process involved adversarial training between the generator and discriminator, along with cycle consistency and identity losses.
*   A sample generated image demonstrates the model's ability to apply some artistic style transfer.

**Learnings and Takeaways:**

*   Building and training GANs, especially CycleGANs, requires careful consideration of the model architecture and training objective.
*   Cycle consistency loss is crucial for enabling unpaired image-to-image translation.
*   Hyperparameter tuning, such as LAMBDA for loss weights, and training for more epochs are likely necessary to achieve higher quality results.
*   The choice of discriminator architecture and its ability to handle different domains (if using a single discriminator) is important.

**Future Improvements:**

*   Train the model for a larger number of epochs.
*   Experiment with different hyperparameters for the optimizers and loss weights.
*   Explore different generator and discriminator architectures.
*   Implement techniques like learning rate scheduling or using a learning rate finder to optimize training.
*   Evaluate the generated images using quantitative metrics if available or through a user study.

**Refernce:**
CycleGAN example [Link](https://www.kaggle.com/code/gauripatil2299/generating-monet-style-paintings-using-gans#5-Generator-&-Discriminator)

##Create Submission Files
"""

#generate 7000 image as test set
def generate_additional_images(generator, dataset, num_images, output_dir):
    count = 0
    for image_batch in dataset.take(num_images):
        for i in range(image_batch.shape[0]):
            if count >= num_images:
                break
            real_photo = image_batch[i:i+1]
            fake_monet = generator(real_photo, training=False)
            # Denormalize the image
            fake_monet = (fake_monet * 0.5 + 0.5) * 255.0
            fake_monet = tf.cast(fake_monet, tf.uint8)
            fake_monet = tf.image.encode_jpeg(fake_monet[0])
            tf.io.write_file(os.path.join(output_dir, f'{count}.jpg'), fake_monet)
            count += 1
        if count >= num_images:
            break

generate_additional_images(generator_g, photo_ds, 7000, temp_images_path)

#create zip file
shutil.make_archive("images", "zip", temp_images_path)

#varify image count
import zipfile
import os

zip_file_path = 'images.zip'
image_count = 0
image_extensions = ['.jpg', '.jpeg', '.png']

with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    for file_info in zip_ref.infolist():
        if not file_info.is_dir():
            _, file_extension = os.path.splitext(file_info.filename)
            if file_extension.lower() in image_extensions:
                image_count += 1

print(f"Total number of images in the zip file: {image_count}")

if 7000 <= image_count <= 10000:
    print("Image count is within the valid range for submission.")
else:
    print("Image count is NOT within the valid range for submission.")