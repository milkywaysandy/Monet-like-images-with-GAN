# I'm Something of a Painter Myself

## GAN Model to Generate Monet-like Images from Scenery Photos

This project utilizes a Generative Adversarial Network (GAN) to transform real-life scenery photos into paintings in the style of Claude Monet. The goal is to explore image-to-image translation using deep learning techniques, specifically focusing on artistic style transfer without requiring paired training data.

## Project Goal

The primary objective of this project is to build and train a CycleGAN model capable of generating images that capture the distinctive style of Monet's paintings when given a real-life photo as input.

## Data

The dataset used for this project contains two sets of images:

*   **Monet's Paintings:** Approximately 300 original Monet paintings.
*   **Real-life Photos:** Approximately 7038 real-life scenery photos.

All images are in .jpg format with a resolution of 256x256 pixels and 3 color channels (RGB). 

## Model Architecture

The project employs a CycleGAN-like architecture, which is well-suited for unpaired image-to-image translation. The model consists of:

*   **Two Generator Networks:**
    *   One generator (`generator_g`) translates photos to Monet-style images.
    *   One generator (`generator_f`) translates Monet-style images back to photos (for cycle consistency).
    Both generators utilize a U-Net-like architecture with skip connections to preserve image details.
*   **Two Discriminator Networks:**
    *   One discriminator (`discriminator_x`) distinguishes between real Monet paintings and fake Monet paintings generated by `generator_g`.
    *   One discriminator (`discriminator_y`) distinguishes between real photos and fake photos generated by `generator_f`.
    Both discriminators are PatchGANs, which classify image patches as real or fake.

## Training

The model is trained using a combination of loss functions:

*   **Adversarial Loss:** Encourages the generators to produce images that are indistinguishable from real images in the target domain, and the discriminators to correctly classify real and fake images.
*   **Cycle Consistency Loss:** Ensures that translating an image from one domain to the other and then back to the original domain results in an image similar to the original. This loss is crucial for unpaired training.
*   **Identity Loss:** Encourages the generators to preserve the identity of an image if it is already in the target domain.

The training process involves optimizing the generators and discriminators simultaneously using the Adam optimizer.

## Getting Started

To run this notebook and train the model:

1.  Clone the repository.
2.  Download the dataset from the [Kaggle competition page](https://www.kaggle.com/competitions/gan-getting-started).
3.  Mount your Google Drive in Colab or upload the dataset to your Colab environment.
4.  Unzip the dataset.
5.  Install the necessary libraries (TensorFlow, TensorFlow Addons, Matplotlib, NumPy, PIL, Shutil).
6.  Run the notebook cells sequentially.

## Future Improvements

*   Train the model for a larger number of epochs.
*   Experiment with different hyperparameters for the optimizers and loss weights.
*   Consider using a more standard CycleGAN architecture with two separate discriminators, one for each domain (Monet and Photo).
*   Explore different generator and discriminator architectures.
*   Implement techniques like learning rate scheduling or using a learning rate finder to optimize training.
*   Evaluate the generated images using quantitative metrics if available or through a user study.


## Acknowledgments

*   The dataset is from the [GAN Getting Started Kaggle competition](https://www.kaggle.com/competitions/gan-getting-started).
